{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goeHHlSnissn",
        "outputId": "987996fc-8d49-4822-a777-d4867615b30e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Collecting flask-cors\n",
            "  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Collecting flask_ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Installing collected packages: flask_ngrok, flask-cors\n",
            "Successfully installed flask-cors-6.0.1 flask_ngrok-0.0.25\n"
          ]
        }
      ],
      "source": [
        "!pip install flask flask-cors spacy flask_ngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlP5R-pakD22",
        "outputId": "279654b4-674e-4b3f-9dc1-c259c64c3112"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K-pTybxmMPO",
        "outputId": "2991ed2a-b89e-4e48-9deb-c0fba3518eab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqXczOeDuWwn",
        "outputId": "e0f37cec-05e9-480f-e7e6-2e85fa222edb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.11-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.11-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Replace below with your actual token\n",
        "ngrok.set_auth_token(\"2z4kW9QCMh8NhfG7kWaijKmC8h2_4xYiLo3UPtEfQ3zjYCzPb\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEa0PPx7m9vJ",
        "outputId": "f4378c3e-3c65-42a4-cea4-2595583822b6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# ---- STEP 1: Compound Phrase Extraction ----\n",
        "def extract_compound_phrases(doc):\n",
        "    compound_map = {}\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"compound\":\n",
        "            head = token.head.i\n",
        "            compound_map.setdefault(head, []).append(token)\n",
        "\n",
        "    phrases = set()\n",
        "    used_token_ids = set()\n",
        "\n",
        "    for head_id, compound_tokens in compound_map.items():\n",
        "        head_token = doc[head_id]\n",
        "        full_phrase = \" \".join([t.text for t in sorted(compound_tokens, key=lambda t: t.i)] + [head_token.text])\n",
        "        phrases.add(full_phrase.lower())\n",
        "        used_token_ids.update([t.i for t in compound_tokens] + [head_id])\n",
        "\n",
        "    return phrases, used_token_ids\n",
        "\n",
        "# ---- STEP 2: Extract Relations ----\n",
        "def extract_relations(doc):\n",
        "    relations = []\n",
        "    for token in doc:\n",
        "        if token.dep_ in (\"ROOT\", \"relcl\", \"xcomp\") and token.pos_ == \"VERB\":\n",
        "            subj = None\n",
        "            obj = None\n",
        "            for child in token.children:\n",
        "                if child.dep_ in (\"nsubj\", \"nsubjpass\"):\n",
        "                    subj = child\n",
        "                elif child.dep_ in (\"dobj\", \"pobj\", \"attr\", \"xcomp\"):\n",
        "                    obj = child\n",
        "            if subj and obj:\n",
        "                relations.append({\n",
        "                    \"source\": subj.text.lower(),\n",
        "                    \"target\": obj.text.lower(),\n",
        "                    \"relation\": token.lemma_.lower()\n",
        "                })\n",
        "    return relations\n",
        "\n",
        "# ---- STEP 3: Keyword Extraction ----\n",
        "def extract_keywords(text):\n",
        "    doc = nlp(text)\n",
        "    keywords = set()\n",
        "\n",
        "    compound_phrases, used_ids = extract_compound_phrases(doc)\n",
        "    keywords.update(compound_phrases)\n",
        "\n",
        "    for chunk in doc.noun_chunks:\n",
        "        if not set([t.i for t in chunk]).issubset(used_ids):\n",
        "            keywords.add(chunk.text.strip().lower())\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        keywords.add(ent.text.strip().lower())\n",
        "\n",
        "    for token in doc:\n",
        "        if token.pos_ in (\"NOUN\", \"PROPN\") and token.i not in used_ids and len(token.text.strip()) > 2:\n",
        "            keywords.add(token.text.strip().lower())\n",
        "\n",
        "    return list(set([kw for kw in keywords if len(kw) > 2]))\n",
        "\n",
        "# ---- STEP 4: Embedding Generator ----\n",
        "def get_embedding(text):\n",
        "    doc = nlp(text)\n",
        "    vecs = [t.vector for t in doc if t.has_vector]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(nlp.vocab.vectors_length)\n",
        "\n",
        "# ---- STEP 5: Smart Cluster Labels ----\n",
        "def infer_cluster_label(keywords):\n",
        "    for kw in keywords:\n",
        "        if any(tag in kw for tag in [\"define\", \"definition\", \"is\", \"refers to\"]):\n",
        "            return \"Definition\"\n",
        "        elif any(tag in kw for tag in [\"use\", \"used\", \"purpose\", \"goal\"]):\n",
        "            return \"Application\"\n",
        "        elif any(tag in kw for tag in [\"method\", \"technique\", \"type\", \"model\"]):\n",
        "            return \"Technique\"\n",
        "    return keywords[0]  # fallback\n",
        "\n",
        "# ---- STEP 6: Clustering ----\n",
        "def cluster_keywords(keywords, n_clusters=5):\n",
        "    embeddings = np.array([get_embedding(kw) for kw in keywords])\n",
        "    valid_idx = np.array([not np.all(v == 0) for v in embeddings])\n",
        "    keywords = [kw for i, kw in enumerate(keywords) if valid_idx[i]]\n",
        "    embeddings = embeddings[valid_idx]\n",
        "\n",
        "    if len(embeddings) < n_clusters:\n",
        "        n_clusters = max(1, len(embeddings))\n",
        "\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "    clustered = defaultdict(list)\n",
        "    for kw, label in zip(keywords, labels):\n",
        "        clustered[label].append(kw)\n",
        "\n",
        "    result = []\n",
        "    for label, group in clustered.items():\n",
        "        cluster_label = infer_cluster_label(group)\n",
        "        result.append({\n",
        "            \"cluster_id\": label,\n",
        "            \"cluster_label\": cluster_label,\n",
        "            \"keywords\": group\n",
        "        })\n",
        "\n",
        "    return result\n",
        "\n",
        "# ---- STEP 7: Mind Map Node Generator ----\n",
        "def extract_mindmap_nodes(text):\n",
        "    doc = nlp(text)\n",
        "    keywords = extract_keywords(text)\n",
        "    clusters = cluster_keywords(keywords, n_clusters=5)\n",
        "    relations = extract_relations(doc)\n",
        "\n",
        "    node_map = {}\n",
        "    nodes = []\n",
        "\n",
        "    for cluster in clusters:\n",
        "        cluster_node = {\n",
        "            \"id\": f\"cluster_{cluster['cluster_id']}\",\n",
        "            \"label\": cluster[\"cluster_label\"],\n",
        "            \"type\": \"cluster\",\n",
        "            \"children\": []\n",
        "        }\n",
        "\n",
        "        for kw in cluster[\"keywords\"]:\n",
        "            kw_id = f\"node_{kw.replace(' ', '_')}\"\n",
        "            cluster_node[\"children\"].append({\n",
        "                \"id\": kw_id,\n",
        "                \"label\": kw,\n",
        "                \"type\": \"keyword\"\n",
        "            })\n",
        "            node_map[kw.lower()] = kw_id\n",
        "\n",
        "        node_map[cluster[\"cluster_label\"].lower()] = cluster_node[\"id\"]\n",
        "        nodes.append(cluster_node)\n",
        "\n",
        "    relation_edges = []\n",
        "    for rel in relations:\n",
        "        src = node_map.get(rel[\"source\"])\n",
        "        tgt = node_map.get(rel[\"target\"])\n",
        "        if src and tgt:\n",
        "            relation_edges.append({\n",
        "                \"source\": src,\n",
        "                \"target\": tgt,\n",
        "                \"label\": rel[\"relation\"]\n",
        "            })\n",
        "\n",
        "    return {\n",
        "        \"nodes\": nodes,\n",
        "        \"relations\": relation_edges\n",
        "    }"
      ],
      "metadata": {
        "id": "FpMAOxYFkbTq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"ThinkSpace Backend is Running 🧠\"\n",
        "\n",
        "@app.route(\"/process\", methods=[\"POST\"])\n",
        "def process_text():\n",
        "    data = request.get_json()\n",
        "    text = data.get(\"text\", \"\")\n",
        "    if not text.strip():\n",
        "        return jsonify({\"error\": \"Empty text provided\"}), 400\n",
        "\n",
        "    try:\n",
        "        result = extract_mindmap_nodes(text)\n",
        "        return jsonify(result)  # ✅ Final correct return\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    port = 5000\n",
        "    public_url = ngrok.connect(port).public_url\n",
        "    print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")\n",
        "    app.run(port=port)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8YVca3klm3i",
        "outputId": "4e404482-e503-4e44-a134-947fe41438c3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * ngrok tunnel \"https://3ad7-34-16-252-90.ngrok-free.app\" -> \"http://127.0.0.1:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "WARNING:pyngrok.process.ngrok:t=2025-06-27T14:41:12+0000 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context deadline exceeded\"\n",
            "INFO:werkzeug:127.0.0.1 - - [27/Jun/2025 14:41:57] \"OPTIONS /process HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [27/Jun/2025 14:41:58] \"POST /process HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [27/Jun/2025 14:50:41] \"OPTIONS /process HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [27/Jun/2025 14:50:42] \"POST /process HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    }
  ]
}